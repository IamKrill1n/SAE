{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "# from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\sae_lens\\sae.py:146: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\Lib\\site-packages\\sae_lens\\sae.py:644: UserWarning: norm_scaling_factor not found for anhtu77/sae-tiny-stories-1L-21M and sae_ex32, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sae = SAE.from_pretrained(\n",
    "    release=\"anhtu77/sae-tiny-stories-1L-21M\",\n",
    "    sae_id=\"sae_ex32\",\n",
    "    device=device,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n",
      "\n",
      "Registered hooks:\n",
      "['hook_embed', 'hook_pos_embed', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_result', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_attn_in', 'blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.hook_mlp_in', 'blocks.0.hook_attn_out', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_pre', 'blocks.0.hook_resid_mid', 'blocks.0.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n"
     ]
    }
   ],
   "source": [
    "# Load the model from huggingface hub\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    # \"roneneldan/TinyStories-Instruct-28M\",\n",
    "    \"tiny-stories-1L-21M\",\n",
    "    # \"gpt2-small\",\n",
    "    device=device,\n",
    ")  # This will wrap huggingface models and has lots of nice utilities.\n",
    "# Print out the model architecture\n",
    "# print(model)\n",
    "\n",
    "# Optionally, inspect the registered hooks in the model\n",
    "print(\"\\nRegistered hooks:\")\n",
    "print(list(model.hook_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n",
    "# for i in range(5):\n",
    "#     display(\n",
    "#         model.generate(\n",
    "#             \"I am a slave, my skin color is\",\n",
    "#             stop_at_eos=False,  # avoids a bug on MPS\n",
    "#             temperature=0.1,\n",
    "#             verbose=False,\n",
    "#             max_new_tokens=10,\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little', ' girl', ' named', ' Lily', '.', ' She', ' lived', ' in', ' a', ' big', ',', ' happy', ' little', ' town', '.', ' On', ' her', ' big', ' adventure', ',']\n",
      "Tokenized answer: [' Lily']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.82</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.62</span><span style=\"font-weight: bold\">% Token: | Lily|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.82\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m8.62\u001b[0m\u001b[1m% Token: | Lily|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.00 Prob: 76.18% Token: | she|\n",
      "Top 1th token. Logit: 18.82 Prob:  8.62% Token: | Lily|\n",
      "Top 2th token. Logit: 18.16 Prob:  4.45% Token: | there|\n",
      "Top 3th token. Logit: 17.00 Prob:  1.39% Token: | the|\n",
      "Top 4th token. Logit: 16.76 Prob:  1.10% Token: | her|\n",
      "Top 5th token. Logit: 16.61 Prob:  0.94% Token: | all|\n",
      "Top 6th token. Logit: 16.56 Prob:  0.90% Token: | everyone|\n",
      "Top 7th token. Logit: 16.04 Prob:  0.53% Token: | things|\n",
      "Top 8th token. Logit: 16.04 Prob:  0.53% Token: | they|\n",
      "Top 9th token. Logit: 16.03 Prob:  0.53% Token: | people|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Lily'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Lily'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformer_lens.utils import test_prompt\n",
    "\n",
    "# # Test the model with a prompt\n",
    "# test_prompt(\n",
    "#     \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little town. On her big adventure,\",\n",
    "#     \" Lily\",\n",
    "#     model,\n",
    "#     prepend_space_to_answer=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>29362</th>\n",
       "      <th>10235</th>\n",
       "      <th>27700</th>\n",
       "      <th>20744</th>\n",
       "      <th>29533</th>\n",
       "      <th>23730</th>\n",
       "      <th>5573</th>\n",
       "      <th>10167</th>\n",
       "      <th>13510</th>\n",
       "      <th>2605</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>fright</td>\n",
       "      <td>again</td>\n",
       "      <td>lot</td>\n",
       "      <td>repay</td>\n",
       "      <td>protective</td>\n",
       "      <td>blocking</td>\n",
       "      <td>Restore</td>\n",
       "      <td>Anne</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mind</td>\n",
       "      <td>surprise</td>\n",
       "      <td>afterward</td>\n",
       "      <td>tattoo</td>\n",
       "      <td>motivate</td>\n",
       "      <td>territorial</td>\n",
       "      <td>useless</td>\n",
       "      <td>She</td>\n",
       "      <td>Tara</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>excitement</td>\n",
       "      <td>forever</td>\n",
       "      <td>blob</td>\n",
       "      <td>reduce</td>\n",
       "      <td>cruel</td>\n",
       "      <td>shedding</td>\n",
       "      <td>They</td>\n",
       "      <td>Sara</td>\n",
       "      <td>she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wear</td>\n",
       "      <td>delight</td>\n",
       "      <td>who</td>\n",
       "      <td>spac</td>\n",
       "      <td>ensure</td>\n",
       "      <td>precious</td>\n",
       "      <td>usable</td>\n",
       "      <td>Their</td>\n",
       "      <td>Ann</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recognize</td>\n",
       "      <td>joy</td>\n",
       "      <td>now</td>\n",
       "      <td>funnel</td>\n",
       "      <td>earn</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>murky</td>\n",
       "      <td>Support</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Lawrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>notice</td>\n",
       "      <td>hesitate</td>\n",
       "      <td>.</td>\n",
       "      <td>threat</td>\n",
       "      <td>prevent</td>\n",
       "      <td>stressed</td>\n",
       "      <td>wary</td>\n",
       "      <td>Mom</td>\n",
       "      <td>Mara</td>\n",
       "      <td>Miguel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feel</td>\n",
       "      <td>action</td>\n",
       "      <td>with</td>\n",
       "      <td>ghost</td>\n",
       "      <td>protect</td>\n",
       "      <td>mad</td>\n",
       "      <td>resisting</td>\n",
       "      <td>Gener</td>\n",
       "      <td>Abby</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>lier</td>\n",
       "      <td>wizard</td>\n",
       "      <td>prove</td>\n",
       "      <td>strict</td>\n",
       "      <td>scor</td>\n",
       "      <td>But</td>\n",
       "      <td>Clara</td>\n",
       "      <td>Dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>agree</td>\n",
       "      <td>shock</td>\n",
       "      <td>today</td>\n",
       "      <td>monster</td>\n",
       "      <td>commemorate</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>dehyd</td>\n",
       "      <td>Buck</td>\n",
       "      <td>Anna</td>\n",
       "      <td>Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>understand</td>\n",
       "      <td>despair</td>\n",
       "      <td>.\"</td>\n",
       "      <td>begg</td>\n",
       "      <td>inspire</td>\n",
       "      <td>cute</td>\n",
       "      <td>struggling</td>\n",
       "      <td>Fix</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>Ginny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         29362          10235       27700     20744         29533  \\\n",
       "0         care         fright       again       lot         repay   \n",
       "1         mind       surprise   afterward    tattoo      motivate   \n",
       "2         like     excitement     forever      blob        reduce   \n",
       "3         wear        delight         who      spac        ensure   \n",
       "4    recognize            joy         now    funnel          earn   \n",
       "5       notice       hesitate           .    threat       prevent   \n",
       "6         feel         action        with     ghost       protect   \n",
       "7         know   anticipation        lier    wizard         prove   \n",
       "8        agree          shock       today   monster   commemorate   \n",
       "9   understand        despair          .\"      begg       inspire   \n",
       "\n",
       "          23730        5573      10167   13510      2605   \n",
       "0    protective     blocking   Restore    Anne       they  \n",
       "1   territorial      useless       She    Tara         he  \n",
       "2         cruel     shedding      They    Sara        she  \n",
       "3      precious       usable     Their     Ann        the  \n",
       "4       annoyed        murky   Support   Alice   Lawrence  \n",
       "5      stressed         wary       Mom    Mara     Miguel  \n",
       "6           mad    resisting     Gener    Abby         it  \n",
       "7        strict         scor       But   Clara     Dennis  \n",
       "8       unhappy        dehyd      Buck    Anna      Paige  \n",
       "9          cute   struggling       Fix   Daisy      Ginny  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by getting the top 10 logits for each feature\n",
    "projection_onto_unembed = sae.W_dec @ model.W_U\n",
    "\n",
    "\n",
    "# get the top 10 logits.\n",
    "vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)\n",
    "\n",
    "# get 10 random features\n",
    "random_indices = torch.randint(0, projection_onto_unembed.shape[0], (10,))\n",
    "\n",
    "# Show the top 10 logits promoted by those features\n",
    "top_10_logits_df = pd.DataFrame(\n",
    "    [model.to_str_tokens(i) for i in inds[random_indices]],\n",
    "    index=random_indices.tolist(),\n",
    ").T\n",
    "top_10_logits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ActivationStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=2,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "\n",
    "# A very handy function Neel wrote to get context around a feature activation\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=3, model=model):\n",
    "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
    "    unique_token = [\n",
    "        [f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens\n",
    "    ]\n",
    "\n",
    "    context = []\n",
    "    prompt = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p - len_prefix) : p])\n",
    "            if p == tokens.shape[1] - 1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(\n",
    "                    str_tokens[b][p + 1 : min(tokens.shape[1] - 1, p + 1 + len_suffix)]\n",
    "                )\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            prompt.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(\n",
    "        dict(\n",
    "            str_tokens=list_flatten(str_tokens),\n",
    "            unique_token=list_flatten(unique_token),\n",
    "            context=context,\n",
    "            prompt=prompt,\n",
    "            pos=pos,\n",
    "            label=label,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max activating examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Max Activating Examples   </span>\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Act   </span>┃<span style=\"font-weight: bold\"> Sequence         </span>┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> one</span> two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> two</span> three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> three</span>' │\n",
       "└───────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  Max Activating Examples   \u001b[0m\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mAct  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '\u001b[1;4;32m one\u001b[0m two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one\u001b[1;4;32m two\u001b[0m three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two\u001b[1;4;32m three\u001b[0m' │\n",
       "└───────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "    return torch.stack((rows, cols), dim=1)\n",
    "\n",
    "\n",
    "x = torch.arange(40, device=device).reshape((2, 20))\n",
    "x[0, 10] += 50  # 2nd highest value\n",
    "x[0, 11] += 100  # highest value\n",
    "x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "assert top_indices.tolist() == [[0, 11], [0, 10]]\n",
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + torch.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle\n",
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "\n",
    "        # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "\n",
    "    return sorted(data, key=lambda x: x[0], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# # finding max activating examples is a bit harder. To do this we need to calculate feature activations for a large number of tokens\n",
    "# feature_list = torch.randint(0, sae.cfg.d_sae, (100,))\n",
    "# examples_found = 0\n",
    "# all_fired_tokens = []\n",
    "# all_feature_acts = []\n",
    "# all_reconstructions = []\n",
    "# all_token_dfs = []\n",
    "\n",
    "# total_batches = 100\n",
    "# batch_size_prompts = activation_store.store_batch_size_prompts\n",
    "# batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
    "# pbar = tqdm(range(total_batches))\n",
    "# for i in pbar:\n",
    "#     tokens = activation_store.get_batch_tokens()\n",
    "#     tokens_df = make_token_df(tokens)\n",
    "#     tokens_df[\"batch\"] = i\n",
    "\n",
    "#     flat_tokens = tokens.flatten()\n",
    "\n",
    "#     _, cache = model.run_with_cache(\n",
    "#         tokens, stop_at_layer=sae.cfg.hook_layer + 1, names_filter=[sae.cfg.hook_name]\n",
    "#     )\n",
    "#     sae_in = cache[sae.cfg.hook_name]\n",
    "#     feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "#     feature_acts = feature_acts.flatten(0, 1)\n",
    "#     fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
    "#     fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
    "#     reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
    "\n",
    "#     token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
    "#     all_token_dfs.append(token_df)\n",
    "#     all_feature_acts.append(feature_acts[fired_mask][:, feature_list])\n",
    "#     all_fired_tokens.append(fired_tokens)\n",
    "#     all_reconstructions.append(reconstruction)\n",
    "\n",
    "#     examples_found += len(fired_tokens)\n",
    "#     # print(f\"Examples found: {examples_found}\")\n",
    "#     # update description\n",
    "#     pbar.set_description(f\"Examples found: {examples_found}\")\n",
    "\n",
    "# # flatten the list of lists\n",
    "# all_token_dfs = pd.concat(all_token_dfs)\n",
    "# all_fired_tokens = list_flatten(all_fired_tokens)\n",
    "# all_reconstructions = torch.cat(all_reconstructions)\n",
    "# all_feature_acts = torch.cat(all_feature_acts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the indices of the maximum activations for feature 0\n",
    "# feature_0_activations = all_feature_acts[:, 0]\n",
    "# max_indices = torch.topk(feature_0_activations, k=5).indices  # Top 5 examples\n",
    "\n",
    "# # Extract the corresponding rows from all_token_dfs\n",
    "# max_activation_examples = all_token_dfs.iloc[max_indices.cpu().numpy()]\n",
    "\n",
    "# # Print the examples\n",
    "# print(max_activation_examples[['str_tokens', 'context']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    ") -> dict[Literal[\"system\", \"user\", \"assistant\"], str]:\n",
    "    \"\"\"\n",
    "    Returns the system, user & assistant prompts for autointerp.\n",
    "    \"\"\"\n",
    "\n",
    "    data = fetch_max_activating_examples(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "    str_formatted_examples = \"\\n\".join(\n",
    "        f\"{i+1}. {''.join(f'<<{tok}>>' if j == buffer else tok for j, tok in enumerate(seq[1]))}\"\n",
    "        for i, seq in enumerate(data)\n",
    "    )\n",
    "    return {\n",
    "        \"system\": \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
    "        \"user\": f\"\"\"The activating documents are given below:\\n\\n{str_formatted_examples}\"\"\",\n",
    "        \"assistant\": \"this neuron fires on\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4ce75bfd914a50ae7a18c5840e4b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing activations for max activating examples:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion 1: ' end punctuation in quotes or sentences.\\n'\n",
      "Completion 2: ' periods at the end of sentences \\n'\n",
      "Completion 3: ' periods followed by double quotes or question marks followed by double quotes\\n'\n",
      "Completion 4: ' ending punctuation such as periods or question marks\\n'\n",
      "Completion 5: ' periods that end a sentence'\n"
     ]
    }
   ],
   "source": [
    "def get_autointerp_explanation(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    "    n_completions: int = 1,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Queries OpenAI's API using prompts returned from `create_prompt`, and returns\n",
    "    a list of the completions.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"GENAI_API_KEY\"),\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "\n",
    "    prompts = create_prompt(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=50,\n",
    "        stream=False,\n",
    "    )\n",
    "    return [choice.message.content for choice in result.choices]\n",
    "\n",
    "completions = get_autointerp_explanation(model, sae, activation_store, latent_idx=13510, n_completions=5)\n",
    "for i, completion in enumerate(completions):\n",
    "    print(f\"Completion {i+1}: {completion!r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
