\documentclass{article}

% --- Recommended Packages ---
\usepackage[utf8]{inputenc} % Allows UTF-8 characters
\usepackage[T1]{fontenc}    % Standard font encoding
\usepackage{amsmath}        % For mathematical equations
\usepackage{amssymb}        % For additional math symbols
\usepackage{graphicx}       % For including images/figures
\usepackage{booktabs}       % For professional tables
\usepackage{caption}        % For better control over captions
\usepackage{subcaption}     % For subfigures
\usepackage{url}            % For handling URLs
\usepackage{hyperref}       % For clickable links (citations, URLs, contents)
\usepackage{cleveref}       % For smart cross-referencing (e.g., Fig. 1, Table 2)
\usepackage{enumitem}       % For customizing lists
\usepackage{geometry}       % For adjusting page margins
\usepackage{setspace}       % For adjusting line spacing
\usepackage{parskip}
% --- Biblatex for References (Recommended) ---
% You need to compile with: latexmk -xelatex -shell-escape YOUR_FILE.tex
% Or: pdflatex YOUR_FILE.tex -> bibtex YOUR_FILE.aux -> pdflatex YOUR_FILE.tex -> pdflatex YOUR_FILE.tex
\usepackage[backend=bibtex, style=numeric, sorting=nty]{biblatex}
\addbibresource{references.bib} % Create a file named references.bib with your citations

% --- Geometry (adjust as needed) ---
\geometry{a4paper, margin=1in}

% --- Line Spacing (adjust as needed, e.g., 1.5 for theses, 1.0 for papers) ---
%\onehalfspacing
%\doublespacing

% --- Hyperref Setup (optional, customize colours etc.) ---
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=green,
    urlcolor=cyan,
    pdftitle={Find interpretable features in language models},
    pdfauthor={Pham Anh Tu},
}


% --- Title Information ---
\title{Find interpretable features in language models}
\author{Pham Anh Tu\\
        Hanoi University of Science and Technology \\
        20225463 % Optional
        }
\date{\today} % Or a specific date like August 2024

% --- Begin Document ---
\begin{document}

\maketitle

% --- Abstract ---
\begin{abstract}
Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their internal workings remain largely opaque. This opacity poses challenges for understanding, debugging, and improving these models. Sparse Autoencoders (SAEs) have emerged as a promising technique for interpreting the dense internal representations of LLMs by decomposing them into sparse, interpretable features. This project involves training Sparse Autoencoders on the hidden layer activations of transformer models and analyzing learned features. Our analysis aims to identify potentially meaningful features related to syntax, semantics, or other properties captured by these models. I present preliminary results on the training process and provide examples of feature activations, discussing insights gained into the model's internal representations. This work contributes to the broader effort of improving LLM interpretability, paving the way for more reliable and controllable AI systems.

% Keywords (optional)
\vspace{0.5em}
\noindent\textbf{Keywords:} Large Language Models, GPT-2, Interpretability, Sparse Autoencoders, Feature Analysis, NLP.
\end{abstract}

% --- Table of Contents (Optional, but useful for longer reports) ---
% \clearpage % Start table of contents on a new page
% \tableofcontents
% \clearpage % Start the first section on a new page

% --- 1. Introduction ---
\section{Introduction}
% Introduce the context: The rise of LLMs and their power.
% State the problem: LLMs are black boxes, difficult to understand.
% Explain the motivation: Why interpretability is important (debugging, trust, improvement).
% Introduce Sparse Autoencoders (SAEs) as a method for interpretability.
% State your specific goal: Train SAEs on gpt2-small and analyze features.
% Briefly outline your contributions.
% Briefly describe the structure of the paper.

% Example citation: Text from [1] shows... or ...as shown by Smith \cite{Smith2020}.
% Use \cite{key} for single citation, \cite{key1, key2} for multiple.
% For parenthetical citations: (e.g., \cite{Smith2020})

Large Language Models (LLMs) have achieved remarkable performance across a wide spectrum of natural language processing tasks, leading to their widespread deployment. However, their powerful capabilities are embedded within complex, opaque 'black box' systems. The dense, high-dimensional representations within LLMs are largely inaccessible to human understanding, creating significant challenges for debugging errors, ensuring reliability, mitigating biases, and building trust in AI systems, underscoring a critical need for improved interpretability.

Efforts in Mechanistic Interpretability aim to understand neural networks by decomposing them into components interpretable by humans. While some prior work has focused on individual neurons \cite{Olahet al., 2020; Bills et al., 2023}, a key challenge is their often polysemantic nature, activating for several unrelated features. This is frequently attributed to superposition, where models represent more features than the dimension of the activation space allows. Sparse Autoencoders (SAEs) \cite{Cunningham et al., 2023} have emerged as a promising alternative approach for finding more disentangled features. By enforcing a sparsity constraint on the latent layer, SAEs learn a dictionary of features that, when sparsely combined, can reconstruct the dense activations, potentially yielding fewer polysemantic units than individual neurons.

Building on this potential, the primary objective of this project is to empirically investigate the utility of Sparse Autoencoders in understanding the internal representations of transformer-based language models. Specifically, this involves training SAEs on hidden layer activations extracted from small GPT models and analyzing the characteristics and activation patterns of the learned sparse features. Through this analysis, we aim to identify features that exhibit clear correlations with linguistic properties such as syntax, semantics, or other potentially interpretable concepts captured by the model.

The rest of this paper is structured as follows: Section~\ref{sec:background} provides necessary background on LLMs, interpretability techniques, and Sparse Autoencoders. Section~\ref{sec:methodology} details the experimental setup, including data collection, SAE architecture, and training procedure. Section~\ref{sec:experiments} presents the experimental results, including training performance and qualitative/quantitative analysis of the learned features. Section~\ref{sec:discussion} discusses the findings, limitations, and implications. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines future work.

% Use \cref{sec:background} for section cross-referencing. Cleveref automatically adds "Section".

% --- 2. Methodology ---
\section{Methodology}
\label{sec:methodology}
% Detail how you conducted the research and experiments.
% 3.2. Sparse Autoencoder Architecture and Training
% Specifics of your SAE model:
% - Size of the input layer (dimension of gpt2-small activation).
% - Size of the sparse hidden layer (should be significantly larger than input for overcompleteness).
% - Activation function used in the encoder.
% - Decoder structure (linear or non-linear?).
% - Loss function details (which reconstruction loss, which sparsity penalty).
% - Optimizer used (e.g., Adam, AdamW).
% - Learning rate schedule.
% - Batch size.
% - Number of training steps/epochs.
% - Hardware used (GPU model, number of GPUs).
% - How was the sparsity coefficient ($\lambda$ in Eq.~\ref{eq:sae_loss}) chosen or tuned?

\subsection{Setup}

The training of the autoencoders was performed on hidden layer activations derived from two transformer models:
\begin{itemize}
    \item The residual stream of a one-layer Tiny-Stories transformer.
    \item All 12 layers of a GPT-2 small model.
\end{itemize}
Activations were extracted using the Skylion007/openwebtext dataset for GPT-2 and apollo-research/roneneldan-TinyStories-tokenizer-gpt2 for Tiny-Stories. A fixed context size of 256 tokens was used across all runs.

Evaluation of the autoencoders focused on three primary metrics:
\begin{itemize}
    \item \textbf{L0 Sparsity:} The average number of non-zero features in the sparse code.
    \item \textbf{Reconstruction MSE:} The Mean Squared Error between the original input activation and the activation reconstructed by the autoencoder.
    \item \textbf{CE Loss Score:} A measure of functional performance, calculated as $(CE_{ablation} - CE_{SAE}) / (CE_{ablation} - CE_{original})$, where $CE$ refers to the language model's cross-entropy loss under different conditions (ablation, using SAE output, using original activations). This score quantifies how well the SAE reconstruction preserves the original model's output quality.
\end{itemize}

Hyperparameters included a sweep of the initial learning rate and learning rate decay during the final 20\% of training. Specific configurations were used based on the autoencoder type:
\begin{itemize}
    \item \textbf{ReLU Autoencoders:} An L1 coefficient warmup was applied for the first 5\% of total training steps.
    \item \textbf{TopK Autoencoders:} The value of $k$ (number of active features) was set to 32 for all experiments.
\end{itemize}

% 3.3. Feature Analysis Techniques
% How did you analyze the learned features?
% - Activation analysis: How to find inputs that strongly activate a specific feature.
% - Feature visualization (if any).
% - Attempts to assign semantic meaning to features.
% - Quantitative metrics for features (e.g., activation frequency, lifetime sparsity).
% - How did you handle dead features?

\subsection{ReLU Autoencoders}

For an input activation vector $x \in \mathbb{R}^d$ from the residual stream and a target latent dimension $n$, we utilize standard ReLU-activated sparse autoencoders, based on architectures like those explored in \cite{Cunningham et al., 2023}. The model consists of an encoder and a decoder.

The encoder maps the input vector $x$ to a latent representation $z \in \mathbb{R}^n$:
\[ z = \text{ReLU}(W_{\text{enc}}x + b_{\text{enc}}) \]
where $W_{\text{enc}} \in \mathbb{R}^{n \times d}$ is the encoding weight matrix, $b_{\text{enc}} \in \mathbb{R}^n$ is the encoder bias.

The decoder reconstructs the input from the latent representation $z$:
\[ \hat{x} = W_{\text{dec}}z + b_{\text{dec}} \]
where $W_{\text{dec}} \in \mathbb{R}^{d \times n}$ is the decoding weight matrix, $b_{\text{dec}} \in \mathbb{R}^d$ is the decoder bias.

Training of the ReLU autoencoder is driven by a composite loss function designed to minimize reconstruction error while promoting sparsity in the latent activation $z$:
\[ \mathcal{L} = \|x - \hat{x}\|^2_2 + \lambda \|z\|_1 \]
Here, $\|x - \hat{x}\|^2_2$ represents the Mean Squared Error (MSE) of the reconstruction. The term $\lambda \|z\|_1$ is an L1 penalty on the latent activation, where $\lambda$ is called the L1 coefficient, a hyperparameter controlling the strength of the sparsity constraint. Minimizing this L1 term encourages most elements of $z$ to be zero for any given input, thereby promoting sparse representations.

\subsection{TopK Autoencoders}

We also employ $k$-sparse autoencoders, as introduced by \cite{Makhzani and Frey, 2013}. This architecture directly enforces sparsity by using a TopK activation function, which ensures that only the $k$ largest values in the encoded vector are kept, while all others are set to zero.

Similar to the ReLU model, the TopK autoencoder takes an input vector $x \in \mathbb{R}^d$ and encodes it into a latent representation $z \in \mathbb{R}^n$. The encoder is defined as:
\[ z = \text{TopK}(W_{\text{enc}}x + b_{\text{enc}}, k) \]
Here, $W_{\text{enc}} \in \mathbb{R}^{n \times d}$ and $b_{\text{enc}} \in \mathbb{R}^n$ serve the same roles as in the ReLU model. The $\text{TopK}(\cdot, k)$ function takes the result of the linear transformation and sets all but its $k$ largest elements to zero, directly controlling the number of active features in $z$.

The decoder structure is identical to that of the ReLU autoencoder, reconstructing the input from the $k$-sparse latent vector $z$:
\[ \hat{x} = W_{\text{dec}}z + b_{\text{dec}} \]
where $W_{\text{dec}} \in \mathbb{R}^{d \times n}$ and $b_{\text{dec}} \in \mathbb{R}^d$.

The training objective for the TopK autoencoder focuses solely on reconstruction accuracy, as sparsity is explicitly managed by the TopK activation function:
\[ \mathcal{L} = \|x - \hat{x}\|^2_2 \]
The loss is simply the Mean Squared Error between the original and reconstructed input vectors.
% --- 3. Experiments and Results ---
\section{Experiments and Results}
\label{sec:experiments}
% Present your findings clearly and objectively.

% 4.1. Training Performance
% Show training curves (e.g., loss over steps, reconstruction error over steps, sparsity level over steps).
% Discuss convergence.
% Report final metrics (e.g., final reconstruction error, average lifetime sparsity).

\subsection{Reconstruction performance}
The reconstruction vs sparsity trade-off is a critical aspect of sparse autoencoder training. In this context, reconstruction refers to the ability of the autoencoder to accurately reconstruct the input activations, typically measured by the Mean Squared Error (MSE) between the original and reconstructed inputs. L0 sparsity, on the other hand, quantifies the average number of non-zero features in the latent representation, with lower values indicating sparser representations.

As shown in \cref{fig:training_loss}, there is an inherent trade-off between these two objectives. Increasing sparsity (lower L0) often comes at the cost of higher reconstruction error, as the autoencoder has fewer active features to represent the input. Conversely, reducing the sparsity constraint (higher L0) allows for more accurate reconstructions but results in less interpretable, denser representations.

\begin{figure}[!htbp]
    \centering
    % Example: Include a plot of the training loss
    \includegraphics[width=0.45\textwidth]{train_res/mse_l0.png}
    \includegraphics[width=0.45\textwidth]{train_res/ce_l0.png}
    \caption{Reconstruction vs L0 trade off}
    \label{fig:training_loss}
\end{figure}
% Use \cref{fig:training_loss} to reference the figure.

% 4.2. Qualitative Feature Analysis
% Provide examples of features and the inputs that activate them.
% Try to interpret what these features might represent (e.g., "is_negation_token", "is_location_entity", "is_past_tense_verb").
% Show examples of inputs and corresponding feature activations.

\subsection{Qualitative Feature Analysis}
[Present examples of interesting features and their activations.]

Table~\ref{tab:feature_examples} shows a few examples of features and input snippets that maximally activate them.

\begin{table}[!htbp]
    \centering
    \caption{Examples of Learned Features and Activating Inputs}
    \label{tab:feature_examples}
    \begin{tabular}{cll}
        \toprule
        Feature Index & Potential Interpretation & Example Input Snippets \\
        \midrule
        123           & Negation marker          & "... not good...", "... don't like...", "... never saw..." \\
        456           & Location indicator       & "... in London", "... to Paris", "... near the coast" \\
        789           & Past tense verb          & "... he walked", "... she said", "... they ran" \\
        \bottomrule
    \end{tabular}
    % Use \cref{tab:feature_examples} to reference the table.
\end{table}

% 4.3. Quantitative Feature Analysis (Optional, but good)
% Statistics about the learned features.
% - Distribution of activation frequencies.
% - Number of dead features.
% - How many features could be assigned a plausible interpretation?
% - Any correlations between features?

\subsection{Quantitative Feature Analysis}
[Present statistics about the learned features.]

% --- 5. Discussion ---
\section{Discussion}
\label{sec:discussion}
% Interpret your results.
% What do the learned features tell us about gpt2-small's representations?
% Were the SAEs successful in finding interpretable features?
% Discuss the challenges faced during training and analysis.
% Address the limitations of your study (e.g., limited computational resources, focus on one layer, specific analysis methods).
% Compare your findings (if possible) to prior work on SAEs or GPT-2 interpretability.

[Discuss the meaning and implications of your results.]

% --- 6. Conclusion ---
\section{Conclusion}
\label{sec:conclusion}
% Summarize the main objectives and key findings.
% Reiterate the contribution of your work.
% Briefly mention the potential impact.
% Point towards future research directions.

[Summarize the work and conclude.]

% --- Future Work (Optional - can be a subsection of Discussion or Conclusion) ---
\section{Future Work}
% What are the next steps?
% - Training SAEs on other layers or other models.
% - More sophisticated feature analysis techniques.
% - Using the learned features for downstream tasks (e.g., controlling model behavior).
% - Improving the efficiency or effectiveness of SAE training.

[Outline potential future research directions.]

% --- References ---
% This section is automatically generated by biblatex based on your .bib file and \cite commands.
\printbibliography

% --- Appendices (Optional) ---
% For supplementary material:
% - Detailed experimental configurations.
% - Longer lists of features.
% - Code snippets (consider platforms like GitHub instead for full code).
% - Additional figures or tables.

\appendix
\section{Appendix A: Experimental Setup Details}
[Provide detailed parameters used for training the SAE.]

\section{Appendix B: Examples of Feature Activations}
[More examples of input snippets activating specific features.]

\end{document}